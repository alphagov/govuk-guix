#!/usr/bin/env ruby

require 'etc'
require 'uri'
require 'json'
require 'open3'
require 'optparse'
require 'shellwords'
require 'fileutils'

def data_directory_base_url
  ENV['GOVUK_GUIX_DATA_DIRECTORY_BASE_URL'] || 's3://govuk-development-data-test'
end

def cache_directory
  base = ENV['XDG_CACHE_HOME'] || "#{ENV['HOME']}/.cache"

  "#{base}/govuk-guix/development-data"
end

def format_seconds(seconds)
  if seconds >= 120
    minutes = seconds.to_i / 60
    "%i minutes, %i seconds" % [minutes, seconds.modulo(60).round]
  elsif seconds >= 1
    "%i seconds" % seconds.round
  else
    "less than a second"
  end
end

def pipe_command(*commands)
  command = 'set -o pipefail; ' + commands.join(' | ')

  # Sometimes, /bin/sh isn't bash, including in the Ubuntu govuk-puppet
  # Development VM, so use /bin/bash explicitly if it exists as bash supports
  # pipefail.
  if File.exist? '/bin/bash'
    "/bin/bash -c #{Shellwords.escape(command)}"
  else
    # If /bin/bash doesn't exist, fallback to hoping that /bin/sh supports
    # pipefail.
    command
  end
end

def aws_s3(*args)
  status = system(*(%w[govuk aws --profile govuk-integration -- aws s3] + args))

  unless status
    STDERR.puts "error: aws s3 command failed: #{args.join(' ')}"
    exit 1
  end
end

def aws_s3_download_and_unpack_tar(target, directory, size)
  temp_directory = "#{directory}.temp"
  if Dir.exist? temp_directory
    system('chmod', '-R', 'u+w', temp_directory)
    FileUtils.rm_r temp_directory
  end
  FileUtils.mkdir_p temp_directory

  command = pipe_command(
    "govuk aws --profile govuk-integration -- aws s3 cp #{target} -",
    "pv --size=#{size}",
    'xz -d -T0',
    "tar --extract --directory #{temp_directory}"
  )

  status = system(command)

  unless status
    STDERR.puts "error: aws s3 command failed: #{command}"
    exit 1
  end

  system('chmod', 'u+w', temp_directory)
  File.rename temp_directory, directory
end

def data_storage_directory
  if Dir.exists? '/var/govuk'
    directory = '/var/govuk/development-data'
  elsif Dir.exists? "#{ENV['HOME']}/govuk"
    directory = "#{ENV['HOME']}/govuk/development-data"
  else
    directory = "#{cache_directory}"
  end

  FileUtils.mkdir_p directory
  directory
end

def filename_for_local_extract(extract)
  extract_data_uri = URI(
    File.join(data_directory_base_url, 'data-extracts', extract[:url])
  )

  case extract_data_uri.scheme
  when "file"
    extract_data_uri.path
  when "s3"
    suffix_to_remove = extract[:directory?] ? '.tar.xz' : ''
    File.join(
      data_storage_directory,
      extract[:date],
      File.basename(extract[:url], suffix_to_remove)
    )
  when "http"
  when "https"
  when "ssh"
    raise "#{base_uri.scheme} not yet supported"
  else
    STDERR.puts "error: unrecognised URI scheme #{base_uri.scheme}"
    exit 1
  end
end

def extract_exists_locally?(extract)
  # TODO: Record the expected hash of the extract, and check if the recorded
  # hash matches that in the index. If it doesn't, it means that the file has
  # changed, and should be downloaded again.
  File.exist? filename_for_local_extract(extract)
end

def retrieve_index
  base_uri = URI(data_directory_base_url)

  case base_uri.scheme
  when "file"
    data_extracts_index = "#{base_uri.path}data-extracts/index.json"
    JSON.parse(File.read(data_extracts_index))
  when "s3"
    aws_s3('cp', "#{data_directory_base_url}/data-extracts/index.json", "#{cache_directory}/data-extracts/")
    JSON.parse(File.read("#{cache_directory}/data-extracts/index.json"))
  when "http"
  when "https"
  when "ssh"
    raise "#{base_uri.scheme} not yet supported"
  else
    puts "error: unrecognised URI scheme #{base_uri.scheme}"
  end
end

def get_local_path_to_extract_data(extract, dry_run)
  extract_data_uri = URI(
    File.join(data_directory_base_url, 'data-extracts', extract[:url])
  )

  case extract_data_uri.scheme
  when "file"
    extract_data_uri.path
  when "s3"
    resulting_filename = filename_for_local_extract(extract)
    storage_directory = File.dirname(resulting_filename)

    unless File.exist?(resulting_filename)
      if dry_run
        puts "would download #{extract_data_uri} to #{storage_directory}/"
      else
        puts "downloading #{extract_data_uri} to #{storage_directory}/\n"
        if extract[:directory?]
          aws_s3_download_and_unpack_tar(
            extract_data_uri.to_s,
            resulting_filename,
            extract[:size]
          )
        else
          aws_s3('cp', extract_data_uri.to_s, "#{storage_directory}/")
        end
      end
    else
      puts "using existing file for #{extract_data_uri}"
    end

    resulting_filename
  when "http"
  when "https"
  when "ssh"
    raise "#{base_uri.scheme} not yet supported"
  else
    STDERR.puts "error: unrecognised URI scheme #{base_uri.scheme}"
    exit 1
  end
end

def symbolize_keys(h)
  h.each_with_object({}) do |(key, value), result|
    result[key.to_sym] = if value.is_a?(Hash)
                           symbolize_keys(value)
                         else
                           value
                         end
  end
end

def retrieve_extracts
  index_data = retrieve_index

  index_data['extracts'].map do |extract_hash|
    symbolize_keys(extract_hash)
  end
end

FILTER_OPTIONS = %i(services_and_variants database before_date after_date).freeze

def filtered_extracts(options)
  sort_extracts(
    filter_extracts(
      retrieve_extracts,
      **options.select { |k| FILTER_OPTIONS.include? k }
    )
  )
end

def group_extracts(field, extracts)
  extracts.each_with_object(Hash.new { |h,k| h[k] = [] }) do |extract, result|
    key = extract[field]
    if key.kind_of?(Hash)
      key = key.keys
    elsif !key.kind_of?(Array)
      key = [key]
    end

    key.each do |key_value|
      result[key_value].push(extract)
    end
  end
end

def sort_extracts(extracts)
  extracts.sort_by do |extract|
    [extract[:date], extract.dig(:variant, :properties, :priority)]
  end
end

def filter_extracts(extracts,
                    services_and_variants: false,
                    database: false,
                    before_date: false,
                    after_date: false)
  extracts.select do |extract|
    next if (
        extract[:database] == 'mongo' &&
        # mongorestore in the govuk-puppet devleopment VM doesn't support the
        # archive format yet
        extract.dig(:variant, :name) == 'archive'
      )

    next if database and !database.include? extract[:database]
    next if before_date and extract[:date] > before_date
    next if after_date and extract[:date] < after_date

    if services_and_variants
      services_and_variants_split = services_and_variants
                                      .map do |service_and_variant|
        service_and_variant.split(':')
      end

      # Skip if it doesn't match any of the service variant pairs
      next if services_and_variants_split.none? do |(service, variant)|
        extract[:services].keys.include?(service.to_sym) &&
          (variant.nil? || extract[:variant][:name] == variant)
      end
    end

    true
  end
end

def all_services(extracts)
  extracts.map(&:services).map(&:keys).flatten.unique
end

def list(options)
  extracts = filtered_extracts(options)

  if extracts.empty?
    puts 'govuk: data: No extracts found'
    exit 0
  end

  group_extracts(:services, extracts).each do |service, extracts_for_service|
    puts "service: #{service}"
    group_extracts(
      :database,
      extracts_for_service
    ).each do |database, extracts_for_database|
      puts "  database: #{database}"
      group_extracts(
        :date,
        extracts_for_database
      ).each  do |date, extracts_for_date|
        if extracts_for_date.length > 1
          puts "    - #{date}"
          extracts_for_date.each do |extract|
            puts "      - %<name>s: %<label>s%<downloaded>s" % {
              downloaded: extract_exists_locally?(extract) ? ' (downloaded)' : ''
            }.merge(extract[:variant])
          end
        else
          print "    - #{date}"
          print " (downloaded)" if extract_exists_locally?(
                                     extracts_for_date.first
                                   )
          puts
        end
      end
    end
  end
end

def extracts_to_load_by_service_and_database(extracts)
  group_extracts(
    :services, extracts
  ).each_with_object({}) do |(service, extracts), result|
    result[service] = begin
      group_extracts(
        :database, extracts
      ).each_with_object({}) do |(database, extracts), result|
        # The last extract is the desired one, due to the sorting
        result[database] = sort_extracts(extracts).last
      end
    end
  end
end

def show_extracts_to_load(extracts_by_service_and_database)
  puts "Extracts selected:"
  extracts_by_service_and_database.each do |service, extracts_by_database|
    extracts_by_database.each do |database, extract|
      puts " - #{database} extract from #{extract[:date]} into the #{service} database"
    end
  end
end

def get_decompressor(filename)
  if filename.end_with? 'xz'
    'xz'
  elsif filename.end_with? 'gz'
    'gzip'
  end
end

def load_postgresql_extract(extract, service, options)
  dry_run = options[:dry_run]
  local_file = get_local_path_to_extract_data(extract, dry_run)
  decompressor = get_decompressor(local_file)
  target_database = extract[:services][service][:database]

  if options[:for_govuk_guix_system]
    user = extract[:services][service][:user]
    host_and_port_arguments = "--host=127.0.0.1 --port=#{extract[:services][service][:port]}"
    psql_command = "psql -U postgres #{host_and_port_arguments}"
    pg_restore_command = 'pg_restore'
    owner = extract[:services][service][:user]
  else
    user = 'postgres'
    host_and_port_arguments = ''
    psql_command = 'sudo -E -u postgres psql'
    pg_restore_command = 'sudo -E -u postgres pg_restore'
    owner = 'vagrant'
  end

  commands = [
    "#{psql_command} -c \"DROP DATABASE IF EXISTS \\\"#{target_database}\\\"\"",
    "#{psql_command} -c \"CREATE DATABASE \\\"#{target_database}\\\"\"",
  ]

  format = extract[:variant][:properties][:format]
  if format == 'plain'
    commands.push(
      pipe_command(
        "pv #{local_file}",
        "#{decompressor} -d",
        "#{psql_command} --no-psqlrc --quiet"\
        " --user=#{user} #{target_database}"
      )
    )
  elsif format == 'directory'
    commands.push(
      "#{pg_restore_command} #{host_and_port_arguments} --username=#{user} --no-owner --dbname=#{target_database} --jobs=#{Etc.nprocessors} --exit-on-error #{local_file}"
    )
  else
    STDERR.puts "error: unknown pg_dump format #{format}"
    exit 1
  end

  start_time = Process.clock_gettime(Process::CLOCK_MONOTONIC)

  commands.each do |command|
    if dry_run
      puts "Would run command: #{command}"
    else
      puts "Running command: #{command}"
      unless system(command)
        STDERR.puts "error: creating #{target_database} failed"
        exit 1
      end
    end
  end

  change_database_owner_command = "#{psql_command} -qXAt -c \"ALTER DATABASE \\\"#{target_database}\\\" OWNER TO \\\"#{owner}\\\"\" #{target_database}"

  if dry_run
    puts "Would run command: #{change_database_owner_command}"
  else
    puts "Running #{change_database_owner_command}"
    unless system(change_database_owner_command)
      STDERR.puts "error: command failed: #{change_database_owner_command}"
      exit 1
    end
  end

  alter_owner_queries = {
    table: "SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';",
    view: "SELECT table_name FROM information_schema.views WHERE table_schema = 'public';",
    materialized_view: "SELECT matviewname FROM pg_matviews WHERE schemaname = 'public';",
    sequence: "SELECT sequence_name FROM information_schema.sequences WHERE sequence_schema = 'public';"
  }

  alter_owner_queries.each do |type, query|
    command = "#{psql_command} -qXAt -c \"#{query}\" #{target_database}"

    if dry_run
      puts "Would run command: #{command}"
      next
    end

    object_names, status = Open3.capture2(command)

    unless status.success?
      STDERR.puts "error: command failed: #{command}"
      exit 1
    end

    object_names.each_line do |name|
      change_owner_command = "#{psql_command} -qXAt -c \"ALTER #{type.to_s.gsub('_', ' ').upcase} \\\"#{name.strip}\\\" OWNER TO \\\"#{owner}\\\"\" #{target_database}"

      unless system(change_owner_command)
        STDERR.puts "error: command failed: #{change_owner_command}"
        exit 1
      end
    end
  end

  time_taken = Process.clock_gettime(Process::CLOCK_MONOTONIC) - start_time

  puts "\nSuccessfully created #{target_database} (took #{format_seconds(time_taken)})" unless dry_run
end

def load_mysql_extract(extract, service, options)
  dry_run = options[:dry_run]
  local_file = get_local_path_to_extract_data(extract, dry_run)
  decompressor = get_decompressor(local_file)
  target_database = extract[:services][service][:database]
  mysql_args = "-u root"
  if options[:for_govuk_guix_system]
    mysql_args += " --host=127.0.0.1 --port=#{extract[:services][service][:port]}"
  end
  puts extract[:services][service]

  commands = [
    "mysql #{mysql_args} -e \"DROP DATABASE IF EXISTS #{target_database}\"",
    "mysql #{mysql_args} -e \"CREATE DATABASE #{target_database}\"",
    pipe_command(
      "pv #{local_file}",
      "#{decompressor} -d",
      "mysql #{mysql_args} #{target_database}"
    )
  ]

  start_time = Process.clock_gettime(Process::CLOCK_MONOTONIC)

  commands.each do |command|
    if dry_run
      puts "Would run command: #{command}"
    else
      puts "Running command: #{command}"
      unless system(command)
        STDERR.puts "error: creating #{target_database} failed"
        exit 1
      end

      time_taken = Process.clock_gettime(Process::CLOCK_MONOTONIC) - start_time

      puts "\nSuccessfully created #{target_database} (took #{format_seconds(time_taken)})"
    end
  end
end

def load_mongo_extract(extract, service, options)
  dry_run = options[:dry_run]
  local_file = get_local_path_to_extract_data(extract, dry_run)

  if extract[:variant][:name] == 'archive'
    decompressor = get_decompressor(local_file)
    target_database = extract[:services][service.to_sym][:database]
    from_database = target_database.sub('development', 'production')
    mongorestore_args = "--nsFrom=\"#{from_database}.*\" --nsTo=\"#{target_database}.*\" --archive --drop --quiet"
    if options[:for_govuk_guix_system]
      mongorestore_args += " --host=127.0.0.1:37017"
    end

    command = pipe_command(
      "pv #{local_file}",
      "#{decompressor} -d",
      "mongorestore #{mongorestore_args}"
    )
  elsif extract[:variant][:name] == 'directory'
    target_database = extract[:services][service.to_sym][:database]
    mongorestore_args = "--drop -d #{target_database} #{local_file}"
    if options[:for_govuk_guix_system]
      mongorestore_args += " --host=127.0.0.1:37017"
    end

    command = "mongorestore #{mongorestore_args}"
  else
    STDERR.puts "error: unknown mongo variant #{extract[:variant][:name]}"
    exit 1
  end

  start_time = Process.clock_gettime(Process::CLOCK_MONOTONIC)

  if dry_run
    puts "Would run command: #{command}"
  else
    puts "Running command: #{command}"
    unless system(command)
      STDERR.puts "error: creating #{target_database} failed"
      exit 1
    end

    time_taken = Process.clock_gettime(Process::CLOCK_MONOTONIC) - start_time

    puts "\nSuccessfully created #{target_database} (took #{format_seconds(time_taken)})"
  end
end

def load_extracts(extracts_by_service_and_database, options)
  dry_run = options[:dry_run]

  extracts_by_service_and_database.each do |service, extracts_by_database|
    extracts_by_database.each do |database, extract|
      puts "#{dry_run ? 'Would import' : 'Importing'} extract from #{extract[:date]} into the #{service} database"
      puts

      if database == "postgresql"
        load_postgresql_extract(extract, service, options)
      elsif database == "mysql"
        load_mysql_extract(extract, service, options)
      elsif database == "mongo"
        load_mongo_extract(extract, service, options)
      else
        STDERR.puts "unknown database: #{database}"
        exit 1
      end
    end
  end
end

def load(options)
  extracts = extracts_to_load_by_service_and_database(
    filtered_extracts(options)
  )

  if extracts.empty?
    puts 'govuk: data: No extracts found'
    exit 0
  end

  show_extracts_to_load(extracts)
  puts

  load_extracts(extracts, options)
end

def fetch(options)
  extracts = extracts_to_load_by_service_and_database(
    filtered_extracts(options)
  )

  if extracts.empty?
    puts 'govuk: data: No extracts found'
    exit 0
  end

  show_extracts_to_load(extracts)
  puts

  extracts.each do |service, extracts_by_database|
    extracts_by_database.each do |database, extract|
      local_file = get_local_path_to_extract_data(extract, options[:dry_run])
      puts "govuk: data: finished fetching #{local_file}"
    end
  end
end

def check_prerequsites
  aws_config = "#{ENV['HOME']}/.aws/config"
  unless File.exist? aws_config
    STDERR.puts "error: #{aws_config} not found, see https://docs.publishing.service.gov.uk/manual/aws-cli-access.html for setup instructions"
    exit 1
  end
end

def parse_options
  options = {}

  OptionParser.new do |opt|
    opt.on('--database DATABASE') { |o| options[:database] = o }
    opt.on('--before BEFORE') { |o| options[:before_date] = o }
    opt.on('--after AFTER') { |o| options[:after_date] = o }
    opt.on('--dry-run') { |o| options[:dry_run] = true }
    opt.on('--for-govuk-guix-system') { |o| options[:for_govuk_guix_system] = true }
  end.parse!

  options
end

def main
  system('govuk check-for-govuk-guix-updates')

  options = parse_options

  command = ARGV.shift
  options[:services_and_variants] = ARGV if !ARGV.empty?
  unless command
    STDERR.puts "error: no command specified"
    STDERR.puts "\nvalid commands are:"
    STDERR.puts " - list"
    STDERR.puts " - load"
    STDERR.puts " - fetch"
    exit 1
  end

  check_prerequsites

  case command
  when "list"
    list(options)
  when "load"
    load(options)
  when "fetch"
    fetch(options)
  else
    STDERR.puts "error: command #{command} is not recognised"
    exit 1
  end
rescue Interrupt => e
  exit 1
end

main
